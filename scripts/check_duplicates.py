#!/usr/bin/env python3
"""
å†…å®¹å®¡è®¡è„šæœ¬ï¼šæ£€æŸ¥ä¹¦ç±å†…å®¹ä¸­çš„é‡å¤æ®µè½
"""
import json
import os
import re
from pathlib import Path
from typing import Dict, List, Tuple
from difflib import SequenceMatcher

def strip_html(text: str) -> str:
    """ç§»é™¤ HTML æ ‡ç­¾ï¼Œåªä¿ç•™çº¯æ–‡æœ¬"""
    return re.sub(r'<[^>]+>', '', text)

def normalize_text(text: str) -> str:
    """æ ‡å‡†åŒ–æ–‡æœ¬ï¼šç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ"""
    text = strip_html(text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_sentences(text: str) -> List[str]:
    """å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­"""
    text = normalize_text(text)
    # æ”¯æŒä¸­è‹±æ–‡å¥å­åˆ†å‰²
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.!?]\s*', text)
    return [s.strip() for s in sentences if len(s.strip()) > 10]

def get_paragraphs(text: str) -> List[str]:
    """å°† HTML æ–‡æœ¬åˆ†å‰²æˆæ®µè½"""
    # æŒ‰ <p> æ ‡ç­¾åˆ†å‰²
    paragraphs = re.findall(r'<p>(.*?)</p>', text, re.DOTALL)
    return [normalize_text(p) for p in paragraphs if len(normalize_text(p)) > 20]

def similarity_ratio(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
    return SequenceMatcher(None, text1, text2).ratio()

def find_duplicate_content(short: str, medium: str, long: str) -> Dict:
    """æŸ¥æ‰¾ä¸‰ä¸ªç‰ˆæœ¬ä¹‹é—´çš„é‡å¤å†…å®¹"""
    issues = []
    
    # æ£€æŸ¥ medium å’Œ long ä¹‹é—´çš„é‡å¤æ®µè½
    medium_paras = get_paragraphs(medium)
    long_paras = get_paragraphs(long)
    
    for i, m_para in enumerate(medium_paras):
        for j, l_para in enumerate(long_paras):
            ratio = similarity_ratio(m_para, l_para)
            if ratio > 0.85:  # 85% ä»¥ä¸Šç›¸ä¼¼åº¦
                issues.append({
                    'type': 'medium_long_duplicate',
                    'similarity': ratio,
                    'medium_para': i,
                    'long_para': j,
                    'text_preview': m_para[:100] + '...'
                })
    
    # æ£€æŸ¥ short æ˜¯å¦å®Œå…¨åŒ…å«åœ¨ medium ä¸­
    short_normalized = normalize_text(short)
    medium_normalized = normalize_text(medium)
    
    if short_normalized in medium_normalized:
        issues.append({
            'type': 'short_in_medium',
            'text_preview': short_normalized[:100] + '...'
        })
    
    # æ£€æŸ¥å¥å­çº§åˆ«çš„é‡å¤
    medium_sentences = get_sentences(medium)
    long_sentences = get_sentences(long)
    
    duplicate_sentences = 0
    for m_sent in medium_sentences:
        for l_sent in long_sentences:
            if similarity_ratio(m_sent, l_sent) > 0.9:
                duplicate_sentences += 1
                break
    
    if duplicate_sentences > len(medium_sentences) * 0.3:  # è¶…è¿‡30%çš„å¥å­é‡å¤
        issues.append({
            'type': 'high_sentence_overlap',
            'duplicate_count': duplicate_sentences,
            'total_medium_sentences': len(medium_sentences),
            'ratio': duplicate_sentences / len(medium_sentences)
        })
    
    return {
        'has_issues': len(issues) > 0,
        'issue_count': len(issues),
        'issues': issues
    }

def check_book(book_id: int, locale: str) -> Dict:
    """æ£€æŸ¥å•æœ¬ä¹¦çš„å†…å®¹"""
    file_path = Path(f'content/books/{book_id}/{locale}.json')
    
    if not file_path.exists():
        return {'error': f'File not found: {file_path}'}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        result = find_duplicate_content(
            data.get('summary_short', ''),
            data.get('summary_medium', ''),
            data.get('summary_long', '')
        )
        
        return {
            'book_id': book_id,
            'locale': locale,
            'title': data.get('title', 'Unknown'),
            **result
        }
    except Exception as e:
        return {'error': str(e), 'book_id': book_id, 'locale': locale}

def main():
    """ä¸»å‡½æ•°ï¼šæ£€æŸ¥æ‰€æœ‰ä¹¦ç±"""
    results = []
    
    # æ£€æŸ¥æ‰€æœ‰ 50 æœ¬ä¹¦
    for book_id in range(1, 51):
        for locale in ['zh', 'en']:
            result = check_book(book_id, locale)
            results.append(result)
            
            if result.get('has_issues'):
                print(f"âš ï¸  Book {book_id} ({locale}): Found {result['issue_count']} issues")
            else:
                print(f"âœ… Book {book_id} ({locale}): No issues")
    
    # ä¿å­˜ç»“æœ
    output_path = Path('scripts/audit_results.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡
    total_books = len(results)
    books_with_issues = sum(1 for r in results if r.get('has_issues'))
    total_issues = sum(r.get('issue_count', 0) for r in results)
    
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœï¼š")
    print(f"   æ€»ä¹¦ç±æ•°: {total_books}")
    print(f"   æœ‰é—®é¢˜çš„ä¹¦ç±: {books_with_issues}")
    print(f"   æ€»é—®é¢˜æ•°: {total_issues}")
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return results

if __name__ == '__main__':
    main()
